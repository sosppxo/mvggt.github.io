<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="We propose MVGGT, an efficient end-to-end framework for Multi-view 3D Referring Expression Segmentation (MV-3DRES) that integrates language information into sparse-view geometric reasoning.">
  <meta name="keywords" content="MVGGT, 3D Referring Expression Segmentation, Multimodal, Sparse Views">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation</title>


  <meta property="og:title" content="MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation" />
  <meta property="og:description"
    content="We propose MVGGT, an efficient end-to-end framework for Multi-view 3D Referring Expression Segmentation (MV-3DRES) that integrates language information into sparse-view geometric reasoning." />
  <!-- Twitter automatically scrapes this. Go to https://cards-dev.twitter.com/validator?
      if you update and want to force Twitter to re-scrape. -->
  <meta property="twitter:card" content="summary" />
  <meta property="twitter:title" content="MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation" />
  <meta property="twitter:description"
    content="We propose MVGGT, an efficient end-to-end framework for Multi-view 3D Referring Expression Segmentation (MV-3DRES) that integrates language information into sparse-view geometric reasoning." />
  <!-- <meta property="twitter:image"         content="https://3dmagicpony.github.io/resources/overview.jpg" /> -->

  <!-- MathJax library -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script>


  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-TSQGH8Q0WV"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-TSQGH8Q0WV');
  </script>
  <script type="module" src="https://ajax.googleapis.com/ajax/libs/model-viewer/4.0.0/model-viewer.min.js"></script>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-4 publication-title" style="font-size: 2rem;">MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">Changli Wu<sup>1, 2, â€ </sup>,</span>
              <span class="author-block">Haodong Wang<sup>1, â€ </sup>,</span>
              <span class="author-block">Jiayi Ji<sup>1</sup>,</span>
              <span class="author-block">Yutian Yao<sup>5</sup>,</span>
              <br>
              <span class="author-block">Chunsai Du<sup>4</sup>,</span>
              <span class="author-block">Jihua Kang<sup>4</sup>,</span>
              <span class="author-block">Yanwei Fu<sup>3, 2</sup>,</span>
              <span class="author-block">Liujuan Cao<sup>1, *</sup></span>
            </div>

            <div class="is-size-5 publication-authors" style="margin-bottom: 10px;">
              <span class="author-block" style="margin-right: 10px;"><sup>1</sup>Xiamen University,</span>
              <span class="author-block" style="margin-right: 10px;"><sup>2</sup>Shanghai Innovation Institute,</span>
              <span class="author-block" style="margin-right: 10px;"><sup>3</sup>Fudan University,</span>
              <br>
              <span class="author-block" style="margin-right: 10px;"><sup>4</sup>ByteDance,</span>
              <span class="author-block"><sup>5</sup>Tianjin University of Science and Technology</span>
            </div>

            <div class="is-size-5 publication-authors">
                <span class="author-block" style="font-size: 0.9em;"><sup>â€ </sup>Equal Contribution,</span>
                <span class="author-block" style="font-size: 0.9em;"><sup>*</sup>Corresponding Author</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2601.06874" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/sosppxo/mvggt"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Demo Link -->
                <span class="link-block">
                  <a href="https://huggingface.co/spaces/sosppxo/mvggt"
                    class="external-link button is-normal is-rounded is-dark">
                    <span>ðŸ¤— Demo</span>
                  </a>
                </span>
                <!-- Slides Link -->
                <!-- <span class="link-block">
                  <a href=""
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-presentation"></i>
                    </span>
                    <span>Slides</span>
                  </a>
                </span> -->
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <style>
    .video-border {
      display: inline-block;
      padding: 2px; /* adjust thickness of your 'border' */
      border-radius: 4px;
      background: linear-gradient(45deg, #ff9a9e, #fad0c4); /* try different gradient colors */
    }
    .video-border video {
      display: block;
      border: none;
      border-radius: 4px; /* match the wrapper for a consistent look */
    }
  </style>

  <style>
    #teaser-video {
      max-width: 85%;
      margin: 0 auto;
      display: block;
      border: none; /* remove the solid border */
      border-radius: 4px;
      box-shadow: 0 4px 10px rgba(0, 0, 0, 0.15);
    }
    
    #architecture-img {
      width: 90%;
      margin: 0 auto;
      display: block;
      border: none;
      border-radius: 0; /* removed border radius */
      box-shadow: none; /* removed box shadow */
    }
    
    /* Enhanced style to further reduce space between buttons and teaser video */
    .hero.teaser {
      padding-top: 0;
      margin-top: -3rem; /* Added negative margin to pull the teaser section up */
    }
    .hero.teaser .hero-body {
      padding-top: 0; /* Reduced from 1rem to 0 */
    }
  </style>
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video id="teaser-video" autoplay muted loop playsinline height="100%">
          <source src="./resources/teaser_video_v3_compressed_short.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle" style="text-align: center;"></h2>
      </div>
    </div>
  </section>


  <section class="section" style="padding-top: 1rem;">
    <div class="container is-max-desktop">

      <div class="columns is-centered">
        <div class="column is-full-width has-text-centered">


          <h2 class="title is-4" style="font-weight: 700;">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Most existing 3D referring expression segmentation (3DRES) methods rely on dense, high-quality point clouds, while real-world agents such as robots and mobile phones operate with only a few sparse RGB views and strict latency constraints. We introduce Multi-view 3D Referring Expression Segmentation (MV-3DRES), where the model must recover scene structure and segment the referred object directly from sparse multi-view images. Traditional two-stage pipelines, which first reconstruct a point cloud and then perform segmentation, often yield low-quality geometry, produce coarse or degraded target regions, and run slowly. We propose the <b>Multimodal Visual Geometry Grounded Transformer (MVGGT)</b>, an efficient end-to-end framework that integrates language information into sparse-view geometric reasoning through a dual-branch design. Training in this setting exposes a critical optimization barrier, termed Foreground Gradient Dilution (FGD), where sparse 3D signals lead to weak supervision. To resolve this, we introduce Per-view No-target Suppression Optimization (PVSO), which provides stronger and more balanced gradients across views, enabling stable and efficient learning. To support consistent evaluation, we build MVRefer, a benchmark that defines standardized settings and metrics for MV-3DRES. Experiments show that MVGGT establishes the first strong baseline and achieves both high accuracy and fast inference, outperforming existing alternatives.
            </p>
          </div>
          <br>


          <h2 class="title is-4" style="font-weight: 700;">Task & Benchmark</h2>
          
          <h3 class="title is-5">The MV-3DRES Task</h3>
          <div class="content has-text-justified">
            <p>
              Real-world agents (robots, mobile phones) often operate with <b>sparse RGB views</b> and strict latency constraints, unlike traditional methods that rely on pre-built dense point clouds.
            </p>
            <p>
              We introduce <b>Multi-view 3D Referring Expression Segmentation (MV-3DRES)</b>. The goal is to segment a 3D object described by natural language directly from a few sparse images, without any ground-truth 3D input at inference.
            </p>
            <p>
              <b>Challenges:</b>
              <ul>
                <li><b>Incomplete Geometry:</b> Sparse views lead to noisy and partial 3D reconstruction.</li>
                <li><b>Weak Supervision:</b> The target object occupies a tiny fraction of the 3D space, leading to optimization difficulties.</li>
              </ul>
            </p>
          </div>
          
          <div class="content has-text-centered">
            <img src="./resources/figure1.png" alt="MV-3DRES Task" style="width: 100%; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1);">
            <p class="is-size-7"><b>Figure 1:</b> Comparison of the proposed MV-3DRES task (bottom) against the traditional two-stage reconstruction-then-segmentation pipeline (top) and direct reconstruction failures (middle).</p>
          </div>

          <div class="content has-text-justified">
             <h3 class="title is-5">MVRefer Benchmark</h3>
             <p>
               To standardize evaluation, we built <b>MVRefer</b> based on ScanNet and ScanRefer. It emulates embodied agents by sampling \(N=8\) sparse frames. We provide metrics that decouple grounding accuracy from reconstruction quality, including <b>mIoU<sub>global</sub></b> (3D) and <b>mIoU<sub>view</sub></b> (2D projection).
             </p>
          </div>
          <br>

          <h2 class="title is-4" style="font-weight: 700;">Method: MVGGT</h2>
          
          <div class="content has-text-justified">
            <p>
              We propose the <b>Multimodal Visual Geometry Grounded Transformer (MVGGT)</b>, an end-to-end framework designed for efficiency and robustness.
            </p>
          </div>

          <div class="content has-text-centered">
            <img src="./resources/figure3.png" alt="MVGGT Architecture" style="width: 100%; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1);">
            <p class="is-size-7"><b>Figure 2:</b> Architecture of MVGGT. It features a <b>Frozen Reconstruction Branch</b> (top) that provides a stable geometric scaffold, and a <b>Trainable Multimodal Branch</b> (bottom) that progressively injects language cues into visual features to predict the final 3D segmentation.</p>
          </div>

          <h3 class="title is-5">Dual-Branch Architecture</h3>
          <div class="content has-text-justified">
            <ul>
              <li><b>Frozen Reconstruction Branch:</b> Uses a pre-trained geometry transformer to predict camera poses and depth maps, providing a stable geometric scaffold.</li>
              <li><b>Trainable Multimodal Branch:</b> Injects language features into visual representations. It receives geometric guidance from the frozen branch to align semantics with structure.</li>
            </ul>
          </div>

          <h3 class="title is-5">Solving the "Needle in a Haystack" Problem</h3>
          <div class="columns is-vcentered">
            <div class="column is-6">
              <div class="content has-text-justified">
                <p>
                  <b>The Problem (FGD):</b> In sparse 3D space, the target object is extremely sparse (< 2%), causing the background to dominate the gradients. We call this <b>Foreground Gradient Dilution</b>.
                </p>
                <p>
                  <b>Our Solution (PVSO):</b> We introduce <b>Per-view No-Target Suppression Optimization</b>. Instead of relying solely on weak 3D signals, we enforce supervision in the dense <b>2D image domain</b>.
                </p>
                <ul>
                  <li><b>Concentrated Signal:</b> Foreground takes up ~15% of pixels in 2D views vs < 2% in 3D.</li>
                  <li><b>Balanced Training:</b> We dynamically sample target-visible views to ensure strong gradients.</li>
                </ul>
              </div>
            </div>
             <div class="column is-6">
               <div class="content has-text-centered">
                <img src="./resources/figure2.png" alt="FGD and PVSO" style="width: 100%; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1);">
                <p class="is-size-7"><b>Figure 3:</b> Visualizing the optimization challenge and our solution.</p>
              </div>
            </div>
          </div>
          <br>


          <h2 class="title is-4" style="font-weight: 700;">Qualitative Visualization</h2>
          <div class="content has-text-centered">
            <p>
              Reconstruction of In-the-wild Photos/Videos with VGGT. Click on any thumbnail below to view the 3D reconstruction.
            </p>
          </div>
          <div class="model-viewer-container">
            <model-viewer id="QualitativeResult"
                          src="resources/qualitative/colosseum_v4.glb"
                          alt="3D Model"
                          loading="eager"
                          touch-action="pan-y" environment-image="legacy"
                          camera-orbit="180deg 70deg auto" 
                          zoom-sensitivity="0.2" camera-controls disable-tap min-camera-orbit="auto auto 1m"
                          max-camera-orbit="auto auto 10m" interaction-prompt="none" shadow-intensity="0" ar
                          disable-shadow ar-modes="webxr scene-viewer quick-look"
                          style="width: 90%; height: 90%; background: #ffffff; margin: 0 auto;">
            </model-viewer>
          </div>
          <br>
          <div class="content has-text-centered">
            <div class="thumbnail-container" id="thumbnail-qualitative">
              <!-- <img src="resources/qualitative/college.png" data-glb="resources/qualitative/college_v2.glb"> -->
              <!-- <video src="resources/qualitative/pyramid.mp4" data-glb="resources/qualitative/pyramid.glb" loop playsinline muted></video> -->
              <video src="resources/qualitative/colosseum.mp4" data-glb="resources/qualitative/colosseum_v4.glb" loop playsinline muted></video>
              <video src="resources/qualitative/outdoor_room.mp4" data-glb="resources/qualitative/outdoor_room_v2.glb" loop playsinline muted></video>
              <video src="resources/qualitative/courtroom.mp4" data-glb="resources/qualitative/courtroom_v2.glb" loop playsinline muted></video>
              <img src="resources/qualitative/chess.jpg" data-glb="resources/qualitative/chess_v2.glb">
              <img src="resources/qualitative/indoor.jpg" data-glb="resources/qualitative/indoor_v2.glb">
            </div>  
          </div>  
          <style>
            .thumbnail-container img, .thumbnail-container video {
              transition: all 0.3s ease;
              border: 3px solid transparent;
              cursor: pointer;
            }
            .thumbnail-selected {
              transform: scale(1.2);
              border: 6px solid #79b4f2 !important; 
              box-shadow: 0 0 10px rgba(121, 180, 242, 0.5);
              z-index: 10;
              position: relative;
            }
            
            /* New styles for responsive horizontal gallery */
            .thumbnail-container {
              display: flex;
              flex-wrap: nowrap;
              overflow-x: auto;
              gap: 10px;
              padding: 10px 0;
              -webkit-overflow-scrolling: touch; /* Smooth scrolling on iOS */
              scrollbar-width: thin;
              align-items: center;
            }
            
            .thumbnail-container img, 
            .thumbnail-container video {
              flex: 0 0 auto;
              width: auto;
              height: 120px; /* Consistent height */
              object-fit: cover;
              max-width: none;
            }
            
            /* Custom scrollbar styling */
            .thumbnail-container::-webkit-scrollbar {
              height: 6px;
            }
            
            .thumbnail-container::-webkit-scrollbar-track {
              background: #f1f1f1;
              border-radius: 10px;
            }
            
            .thumbnail-container::-webkit-scrollbar-thumb {
              background: #888;
              border-radius: 10px;
            }
            
            .thumbnail-container::-webkit-scrollbar-thumb:hover {
              background: #555;
            }
            
            /* Adjust for smaller screens */
            @media (max-width: 768px) {
              .thumbnail-container img,
              .thumbnail-container video {
                height: 100px;
              }
            }
          </style>
          <script>
            // The problem is here - you're trying to select an element that doesn't exist
            // document.querySelector('#thumbnail-qualitative img[src="resources/qualitative/college.png"]').classList.add('thumbnail-selected');
            
            // Instead, select the first element that actually exists in your thumbnail container
            document.addEventListener('DOMContentLoaded', function() {
              // Select the first element in the thumbnail container (video or image)
              const firstThumbnail = document.querySelector('#thumbnail-qualitative video, #thumbnail-qualitative img');
              if (firstThumbnail) {
                firstThumbnail.classList.add('thumbnail-selected');
                // If it's a video, play it
                if (firstThumbnail.tagName.toLowerCase() === 'video') {
                  firstThumbnail.play();
                }
              }
              
              document.querySelectorAll('#thumbnail-qualitative img, #thumbnail-qualitative video').forEach(el => {
                // Rest of your click handler code remains the same
                el.addEventListener('click', () => {
                  const glbSrc = el.getAttribute('data-glb');
                  const modelViewer = document.getElementById('QualitativeResult');
                  modelViewer.setAttribute('src', glbSrc);
                  modelViewer.cameraOrbit = "180deg 70deg auto";
                  modelViewer.resetTurntableRotation(0);
                  modelViewer.jumpCameraToGoal();

                  // Remove selection class from all elements
                  document.querySelectorAll('#thumbnail-qualitative img, #thumbnail-qualitative video').forEach(element => {
                      element.classList.remove('thumbnail-selected');
                  });
                  
                  // Add selection class to clicked element
                  el.classList.add('thumbnail-selected');
                  
                  // Play video if it's a video element
                  if (el.tagName.toLowerCase() === 'video') {
                      el.play();
                  }
                  
                  // Pause all other videos
                  document.querySelectorAll('#thumbnail-qualitative video').forEach(video => {
                      if (video !== el) {
                          video.pause();
                          video.currentTime = 0;
                      }
                  });
                });
              });
            });
          </script>

          <br>

          <h2 class="title is-4" style="font-weight: 700;">Qualitative Comparison</h2>
          <div class="content has-text-justified" style="align-self: flex-start;">
            <p>
               VGGT significantly outperforms all other methods across various tasks. Please refer to our paper for quantitative results. Here we also provide a qualitative comparison with DUSt3R and other concurrent works such as Fast3R and FLARE (use the dropdown menu to switch).
            </p>
          </div>


          <div class="model-container" id="model-compare-wrapper">
            <!-- Model 1 Viewer with Label -->
            <div class="model-wrapper-comparison">
              <div class="model-label">VGGT</div>
              <model-viewer id="modelViewerComparison1" loading="eager"
                touch-action="pan-y" environment-image="legacy"
                src="resources/comparison/ours/college.glb"
                zoom-sensitivity="0.2" camera-controls disable-tap min-camera-orbit="auto auto 1m"
                max-camera-orbit="auto auto 10m" interaction-prompt="none" shadow-intensity="0" ar
                style="width: 100%; height: 100%;  background: #ffffff;">
              </model-viewer>
            </div>
            <!-- Model 2 Viewer with Label -->
            <div class="model-wrapper-comparison">
              <div class="model-label">DUSt3R</div>
              <model-viewer id="modelViewerComparison2"loading="eager"
                touch-action="pan-y" environment-image="legacy"
                src="resources/comparison/dust3r/college.glb"
                zoom-sensitivity="0.2" camera-controls disable-tap min-camera-orbit="auto auto 1m"
                max-camera-orbit="auto auto 10m" interaction-prompt="none" shadow-intensity="0" ar
                style="width: 100%; height: 100%; background: #ffffff;">
              </model-viewer>
            </div>
            <!-- Model 3 Viewer with Label -->
            <div class="model-wrapper-comparison">
              <select id="comparisonBaselineSelection" class="dropdown model-label">
                <option value="fast3r">Fast3R</option>
                <option value="flare">FLARE</option>
                <!-- <option value="cut3r">CUT3R</option> -->
              </select>
              <model-viewer id="modelViewerComparison3" loading="eager"
                touch-action="pan-y" environment-image="legacy"
                src="resources/comparison/fast3r/college.glb"
                zoom-sensitivity="0.2" camera-controls disable-tap min-camera-orbit="auto auto 1m"
                max-camera-orbit="auto auto 10m" interaction-prompt="none" shadow-intensity="0" ar
                style="width: 100%; height: 100%; background: #ffffff;">
              </model-viewer>
            </div>
          </div>
          <div class="hero-body" style="padding: 0;">
            <div class="content has-text-centered">
              <div class="thumbnail-container", id="thumbnail-comparison", data-selected-name="Colosseum">                
                <img src="resources/comparison/assets/college.png" name="college">
                <img src="resources/comparison/assets/oil.png" name="oil">
                <video src="resources/comparison/assets/fern.mp4" name="fern" loop playsinline muted></video>
                <video src="resources/comparison/assets/pyramid.mp4" name="pyramid" loop playsinline muted></video>
              </div>  
            </div>  
          </div>


        </div>
      </div>
    </div>
  </section>
  
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title is-4" style="font-weight: 700;">BibTeX</h2>
      <pre><code>@misc{wu2026mvggt,
  Author = {Changli Wu and Haodong Wang and Jiayi Ji and Yutian Yao and Chunsai Du and Jihua Kang and Yanwei Fu and Liujuan Cao},
  Title = {MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation},
  Year = {2026}
}</code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This webpage template is adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>,
              under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0 License</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>
  <script src="static/js/comparison.js"></script>
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      // Set initial camera positions for all model viewers
      const initialModelViewers = [
        document.getElementById('QualitativeResult'),
        document.getElementById('modelViewerComparison1'),
        document.getElementById('modelViewerComparison2'),
        document.getElementById('modelViewerComparison3')
      ];
      
      // Apply consistent initial camera settings to all model viewers
      initialModelViewers.forEach(viewer => {
        if (viewer) {
          viewer.addEventListener('load', () => {
            viewer.cameraOrbit = "180deg 70deg auto";
            if (viewer.resetTurntableRotation) {
              viewer.resetTurntableRotation(0);
            }
            viewer.jumpCameraToGoal();
          });
        }
      });
      
      // Handle comparison thumbnails
      const firstComparisonThumbnail = document.querySelector('#thumbnail-comparison video, #thumbnail-comparison img');
      if (firstComparisonThumbnail) {
        firstComparisonThumbnail.classList.add('thumbnail-selected');
        // If it's a video, play it
        if (firstComparisonThumbnail.tagName.toLowerCase() === 'video') {
          firstComparisonThumbnail.play();
        }
      }
      
      document.querySelectorAll('#thumbnail-comparison img, #thumbnail-comparison video').forEach(el => {
        el.addEventListener('click', () => {
          const name = el.getAttribute('name');
          
          // Update model viewers with the corresponding GLB files
          const modelViewer1 = document.getElementById('modelViewerComparison1');
          const modelViewer2 = document.getElementById('modelViewerComparison2');
          const modelViewer3 = document.getElementById('modelViewerComparison3');
          
          modelViewer1.setAttribute('src', `resources/comparison/ours/${name}.glb`);
          modelViewer2.setAttribute('src', `resources/comparison/dust3r/${name}.glb`);
          
          // For the third viewer, check the dropdown selection
          const baseline = document.getElementById('comparisonBaselineSelection').value;
          modelViewer3.setAttribute('src', `resources/comparison/${baseline}/${name}.glb`);
          
          // Reset camera positions
          [modelViewer1, modelViewer2, modelViewer3].forEach(viewer => {
            viewer.cameraOrbit = "180deg 70deg auto";
            if (viewer.resetTurntableRotation) {
              viewer.resetTurntableRotation(0);
            }
            viewer.jumpCameraToGoal();
          });

          // Remove selection class from all elements
          document.querySelectorAll('#thumbnail-comparison img, #thumbnail-comparison video').forEach(element => {
            element.classList.remove('thumbnail-selected');
          });
          
          // Add selection class to clicked element
          el.classList.add('thumbnail-selected');
          
          // Play video if it's a video element
          if (el.tagName.toLowerCase() === 'video') {
            el.play();
          }
          
          // Pause all other videos
          document.querySelectorAll('#thumbnail-comparison video').forEach(video => {
            if (video !== el) {
              video.pause();
              video.currentTime = 0;
            }
          });
        });
      });
      
      // Handle dropdown change for the third comparison model
      document.getElementById('comparisonBaselineSelection').addEventListener('change', function() {
        const selectedName = document.querySelector('#thumbnail-comparison .thumbnail-selected').getAttribute('name');
        const baseline = this.value;
        document.getElementById('modelViewerComparison3').setAttribute('src', `resources/comparison/${baseline}/${selectedName}.glb`);
      });
    });
  </script>

</body>

</html>
